{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#from jax import grad, jit, random\n",
    "#import jax.numpy as np\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}_{p(b|\\theta)} \\left[ (b - .45)^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob(b, theta):\n",
    "    #return (b - theta)**2\n",
    "    \n",
    "    probs = torch.nn.functional.log_softmax(theta)\n",
    "    b_ohe = torch.zeros_like(probs)\n",
    "    b_ohe[b] = 1\n",
    "    return torch.dot(b_ohe, probs)\n",
    "    #return b*torch.log(theta) + (1 - b)*torch.log(1 - theta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(torch.tensor([0.5, 0.5]), num_samples=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bernoulli(torch.tensor(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_box(x):\n",
    "    return torch.exp(x - x.detach())\n",
    "\n",
    "\n",
    "def objective_reinforce(fun, logprob):\n",
    "    def f(theta, b):\n",
    "        #b = torch.bernoulli(theta)\n",
    "        #bb = magic_box(b)\n",
    "        return fun(b) * magic_box(logprob(b, theta))\n",
    "    return f\n",
    "\n",
    "\n",
    "def fun(b):\n",
    "    return (b - .1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = torch.tensor([0.5, 0.5], requires_grad=True)\n",
    "eta = torch.tensor(0.005, requires_grad=True)\n",
    "\n",
    "def reinforce(theta0, eta):\n",
    "    \"\"\"One step of reinforce gradient estimator\n",
    "    \"\"\"\n",
    "    theta_p = theta0\n",
    "    for i in range(1000):\n",
    "        #bs = 0.1 * torch.randn(1) + theta_p\n",
    "        #bs = torch.bernoulli(theta_p)\n",
    "        bs = torch.multinomial(theta_p, num_samples=1)[0]\n",
    "        theta_p = theta_p - eta*grad(objective_reinforce(fun, logprob)(theta_p, bs), theta_p, create_graph=True)[0]\n",
    "    #loss = fun( 0.1 * torch.randn(1) + theta_p).mean()\n",
    "    loss = fun( torch.multinomial(theta_p, num_samples=1) ).mean()\n",
    "    #loss = fun( torch.bernoulli(theta_p) ).mean()\n",
    "    #losses.append(loss)   \n",
    "    return loss, theta_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-128-28aabe975c0e>:4: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = torch.nn.functional.log_softmax(theta)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0100), tensor([0.9530, 0.0470], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce(theta0, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-128-28aabe975c0e>:4: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = torch.nn.functional.log_softmax(theta)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-03d71b8f1a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         inputs, allow_unused)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "grad(reinforce(theta0, eta)[0], theta0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "prob_dist must be 1 or 2 dim",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-364f6beb1a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mthetas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#print(grad(reinforce(theta0, eta), theta0)[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-bd385a1ca025>\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(theta0, eta)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#bs = 0.1 * torch.randn(1) + theta_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#bs = torch.bernoulli(theta_p)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtheta_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta_p\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_reinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#loss = fun( 0.1 * torch.randn(1) + theta_p).mean()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: prob_dist must be 1 or 2 dim"
     ]
    }
   ],
   "source": [
    "theta0 = torch.tensor(.25, requires_grad=True)\n",
    "eta = torch.tensor(0.1, requires_grad=True)\n",
    "\n",
    "\n",
    "thetas = []\n",
    "\n",
    "for _ in range(1000):\n",
    "\n",
    "    eta = eta - 0.01*grad(reinforce(theta0, eta), eta)[0]\n",
    "    thetas.append(eta)\n",
    "    #print(grad(reinforce(theta0, eta), theta0)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdff29631f0>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAQVElEQVR4nO3cf6zddX3H8eeL1qrg+CXVSNuszUSXbi6gh+KvEDYGlkyp2TAryVxxTpZtLP74Y8P5R2Mx2Ux0Y0uIsQEcOgERNaubWsnQOMNkPa0Ctvy6dggtblwtyphLsPa9P8637u7mtvfc9pZv28/zkZzknO+v+/42l/O85/u9l1QVkqT2nND3AJKkfhgASWqUAZCkRhkASWqUAZCkRi3se4C5OOOMM2r58uV9jyFJx5StW7d+v6oWT19+TAVg+fLlDIfDvseQpGNKku/OtNxLQJLUKAMgSY0yAJLUKAMgSY0yAJLUKAMgSY0yAJLUKAMgSY0yAJLUKAMgSY0yAJLUKAMgSY0yAJLUKAMgSY0yAJLUKAMgSY0yAJLUKAMgSY0yAJLUKAMgSY0yAJLUKAMgSY0yAJLUKAMgSY0yAJLUqLECkGR1kgeTTCS5eob15yfZlmRvksumrVuX5OHusW6GfTcl+fahn4Ik6VDMGoAkC4DrgEuAlcDlSVZO2+xR4Arg5mn7ng6sB84DVgHrk5w2Zf1vAk8fxvySpEM0zieAVcBEVe2sqmeAW4E1Uzeoqkeq6l5g37R93wDcUVV7qupJ4A5gNUCSFwDvAT5wmOcgSToE4wRgCfDYlNe7umXjONi+1wAfBn58sAMkuTLJMMlwcnJyzC8rSZpNLzeBk5wN/EJVfW62batqY1UNqmqwePHiZ2E6SWrDOAHYDSyb8nppt2wcB9r3NcAgySPA14GXJfnqmMeUJM2DcQKwBTgryYoki4C1wKYxj78ZuDjJad3N34uBzVX1kao6s6qWA68HHqqqC+Y+viTpUM0agKraC1zF6M38fuC2qtqeZEOSSwGSnJtkF/AW4KNJtnf77mF0rX9L99jQLZMk9SxV1fcMYxsMBjUcDvseQ5KOKUm2VtVg+nL/EliSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRYwUgyeokDyaZSHL1DOvPT7Ityd4kl01bty7Jw91jXbfsxCT/lOSBJNuT/OX8nI4kaVyzBiDJAuA64BJgJXB5kpXTNnsUuAK4edq+pwPrgfOAVcD6JKd1qz9UVb8InAO8Lsklh3EekqQ5GucTwCpgoqp2VtUzwK3AmqkbVNUjVXUvsG/avm8A7qiqPVX1JHAHsLqqflxVX+n2fQbYBiw9zHORJM3BOAFYAjw25fWubtk4Zt03yanAm4B/HvOYkqR50OtN4CQLgVuAv62qnQfY5sokwyTDycnJZ3dASTqOjROA3cCyKa+XdsvGMdu+G4GHq+raAx2gqjZW1aCqBosXLx7zy0qSZjNOALYAZyVZkWQRsBbYNObxNwMXJzmtu/l7cbeMJB8ATgHeNfexJUmHa9YAVNVe4CpGb9z3A7dV1fYkG5JcCpDk3CS7gLcAH02yvdt3D3ANo4hsATZU1Z4kS4H3Mfqtom1JvpXk94/A+UmSDiBV1fcMYxsMBjUcDvseQ5KOKUm2VtVg+nL/EliSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGjVWAJKsTvJgkokkV8+w/vwk25LsTXLZtHXrkjzcPdZNWf6qJPd1x/zbJDn805EkjWvWACRZAFwHXAKsBC5PsnLaZo8CVwA3T9v3dGA9cB6wClif5LRu9UeAdwBndY/Vh3wWkqQ5WzjGNquAiaraCZDkVmANsGP/BlX1SLdu37R93wDcUVV7uvV3AKuTfBU4uaq+0S3/OPBm4IuHczIH8v7Pb2fH408diUNL0hG38syTWf+mX5r3445zCWgJ8NiU17u6ZeM40L5LuuezHjPJlUmGSYaTk5NjfllJ0mzG+QTQq6raCGwEGAwGdSjHOBLllKRj3TifAHYDy6a8XtotG8eB9t3dPT+UY0qS5sE4AdgCnJVkRZJFwFpg05jH3wxcnOS07ubvxcDmqvoe8FSSV3e//fO7wD8cwvySpEM0awCqai9wFaM38/uB26pqe5INSS4FSHJukl3AW4CPJtne7bsHuIZRRLYAG/bfEAb+CLgemAC+wxG6ASxJmlmqDumyei8Gg0ENh8O+x5CkY0qSrVU1mL7cvwSWpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElqlAGQpEYZAElq1FgBSLI6yYNJJpJcPcP65yb5VLf+7iTLu+WLknwsyX1J7klywZR9Lu+W35vkS0nOmKdzkiSNYdYAJFkAXAdcAqwELk+yctpmbweerKqXAn8NfLBb/g6AqnoFcBHw4SQnJFkI/A3wq1X1K8C9wFXzcD6SpDGN8wlgFTBRVTur6hngVmDNtG3WADd1z28HLkwSRsG4E6CqngB+CAyAdI+Tuu1OBh4/zHORJM3BOAFYAjw25fWubtmM21TVXuBHwAuBe4BLkyxMsgJ4FbCsqn4C/CFwH6M3/pXADYdxHpKkOTrSN4FvZBSMIXAtcBfw0yTPYRSAc4AzGV0Ceu9MB0hyZZJhkuHk5OQRHleS2jFOAHYDy6a8Xtotm3Gb7vr+KcAPqmpvVb27qs6uqjXAqcBDwNkAVfWdqirgNuC1M33xqtpYVYOqGixevHgOpyZJOphxArAFOCvJiiSLgLXApmnbbALWdc8vA+6sqkpyYpKTAJJcBOytqh2MgrEyyf539IuA+w/zXCRJc7Bwtg2qam+Sq4DNwALgxqranmQDMKyqTYyu338iyQSwh1EkAF4EbE6yj9Gb/lu7Yz6e5P3A15L8BPgucMX8npok6WAyugJzbBgMBjUcDvseQ5KOKUm2VtVg+nL/EliSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGmUAJKlRBkCSGjVWAJKsTvJgkokkV8+w/rlJPtWtvzvJ8m75oiQfS3JfknuSXDBln0VJNiZ5KMkDSX5rns5JkjSGhbNtkGQBcB1wEbAL2JJkU1XtmLLZ24Enq+qlSdYCHwR+G3gHQFW9IsmLgC8mObeq9gHvA56oqpclOQE4fV7PTJJ0UON8AlgFTFTVzqp6BrgVWDNtmzXATd3z24ELkwRYCdwJUFVPAD8EBt12vwf8RbduX1V9/3BORJI0N+MEYAnw2JTXu7plM25TVXuBHwEvBO4BLk2yMMkK4FXAsiSndvtdk2Rbkk8nefFMXzzJlUmGSYaTk5Njn5gk6eCO9E3gGxkFYwhcC9wF/JTRpaelwF1V9UrgX4EPzXSAqtpYVYOqGixevPgIjytJ7Zj1HgCwG1g25fXSbtlM2+xKshA4BfhBVRXw7v0bJbkLeAj4AfBj4LPdqk8zuo8gSXqWjPMJYAtwVpIVSRYBa4FN07bZBKzrnl8G3FlVleTEJCcBJLkI2FtVO7owfB64oNvnQmAHkqRnzayfAKpqb5KrgM3AAuDGqtqeZAMwrKpNwA3AJ5JMAHsYRQLgRcDmJPsYfUp465RD/1m3z7XAJPC2+TopSdLsMvph/NgwGAxqOBz2PYYkHVOSbK2qwfTl/iWwJDXKAEhSowyAJDXKAEhSowyAJDXKAEhSowyAJDXKAEhSowyAJDXKAEhSowyAJDXKAEhSowyAJDXKAEhSowyAJDXKAEhSowyAJDXKAEhSowyAJDXKAEhSowyAJDXKAEhSowyAJDXKAEhSo1JVfc8wtiSTwHcPcfczgO/P4zjzxbnmxrnmxrnm5nid6+eravH0hcdUAA5HkmFVDfqeYzrnmhvnmhvnmpvW5vISkCQ1ygBIUqNaCsDGvgc4AOeaG+eaG+eam6bmauYegCTp/2vpE4AkaQoDIEmNOu4DkGR1kgeTTCS5uu959ktyY5Inkny771mmSrIsyVeS7EiyPck7+54JIMnzkvxbknu6ud7f90z7JVmQ5JtJ/rHvWaZK8kiS+5J8K8mw73n2S3JqktuTPJDk/iSvOQpmenn377T/8VSSd/U9F0CSd3ff899OckuS583bsY/newBJFgAPARcBu4AtwOVVtaPXwYAk5wNPAx+vql/ue579krwEeElVbUvyc8BW4M19/5slCXBSVT2d5DnA14F3VtU3+pwLIMl7gAFwclW9se959kvyCDCoqqPqD5uS3AT8S1Vdn2QRcGJV/bDvufbr3jd2A+dV1aH+4el8zbKE0ff6yqr6nyS3AV+oqr+bj+Mf758AVgETVbWzqp4BbgXW9DwTAFX1NWBP33NMV1Xfq6pt3fP/Au4HlvQ7FdTI093L53SP3n96SbIU+A3g+r5nORYkOQU4H7gBoKqeOZre/DsXAt/p+81/ioXA85MsBE4EHp+vAx/vAVgCPDbl9S6OgjezY0WS5cA5wN39TjLSXWr5FvAEcEdVHQ1zXQv8KbCv70FmUMCXk2xNcmXfw3RWAJPAx7rLZtcnOanvoaZZC9zS9xAAVbUb+BDwKPA94EdV9eX5Ov7xHgAdoiQvAD4DvKuqnup7HoCq+mlVnQ0sBVYl6fXSWZI3Ak9U1dY+5ziI11fVK4FLgD/uLjv2bSHwSuAjVXUO8N/A0XRvbhFwKfDpvmcBSHIao6sWK4AzgZOS/M58Hf94D8BuYNmU10u7ZTqI7hr7Z4BPVtVn+55nuu6SwVeA1T2P8jrg0u5a+63AryX5+35H+j/dT49U1RPA5xhdEu3bLmDXlE9vtzMKwtHiEmBbVf1n34N0fh3496qarKqfAJ8FXjtfBz/eA7AFOCvJiq7sa4FNPc90VOtutt4A3F9Vf9X3PPslWZzk1O758xnd2H+gz5mq6r1VtbSqljP63rqzqubtp7PDkeSk7iY+3SWWi4Hef+Osqv4DeCzJy7tFFwK9/1LGFJdzlFz+6TwKvDrJid1/mxcyui83LxbO14GORlW1N8lVwGZgAXBjVW3veSwAktwCXACckWQXsL6qbuh3KmD0U+1bgfu66+0Af15VX+hxJoCXADd1v6FxAnBbVR1Vv3Z5lHkx8LnRewYLgZur6kv9jvQzfwJ8svuhbCfwtp7nAX4WyouAP+h7lv2q6u4ktwPbgL3AN5nH/y3Ecf1roJKkAzveLwFJkg7AAEhSowyAJDXKAEhSowyAJDXKAEhSowyAJDXqfwHc32w22+v1jwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.plot(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Simple higher order autodiff example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    return torch.sum(x**2)\n",
    "\n",
    "def sgd(x0, eta):\n",
    "    return loss(x0 - eta*grad(loss(x0), x0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0400)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(torch.tensor(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0384, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd(torch.tensor(0.2, requires_grad=True), eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1568),)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.tensor(0.2, requires_grad=True)\n",
    "eta = torch.tensor(0.01, requires_grad=True)\n",
    "\n",
    "grad(sgd(x0, eta), eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3920),)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(sgd(x0, eta), x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
